{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: Characterization of Long-Term Runoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats as st\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "from bokeh.plotting import figure, show, output_file\n",
    "from bokeh.models import ColumnDataSource, Band\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.layouts import gridplot\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In Notebook 2, we developed a rating curve for our location of interest based on discrete discharge measurements made during a series of visits, and we applied this stage-discharge relationship (rating curve) to the continous stage recorded at our hydrometric station (the datalogger recording from a pressure transducer).  \n",
    "\n",
    "Recall that our hydrometric station has only been running for a couple of years -- this isn't nearly enough data to estimate the **long term** flow characteristics (daily, seasonal, floods, droughts, etc.).  In this notebook, we look in the vicinity of our project location for other stations where records have been kept for much longer, i.e. several decades longer.  We can use concurrent data from nearby stations to develop a model to estimate flow for periods we didn't actually measure at our project location.\n",
    "\n",
    "First, we'll set up our rating curve as we did in Notebook 2 and rebuild the daily average flow series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the stage data\n",
    "stage_df = pd.read_csv('../../Project_Data/Hidden_Creek_stage_data.csv', parse_dates=['Date'])\n",
    "stage_df.set_index('Date', inplace=True)\n",
    "\n",
    "stage_df.sort_index(inplace=True)\n",
    "stage_df['Value'] = stage_df['Value'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a quick look at what we're dealing with\n",
    "stage_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(stage_df.index, stage_df['Value'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the water level (stage) label is long, \n",
    "# let's create a shortcut reference label\n",
    "stage_label = 'water level (m above 0 flow ref)'\n",
    "flow_label = 'Flow (m^3/s)'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the discharge measurements\n",
    "rc_df = pd.read_csv('../../Project_Data/Project_QH_table_2021.csv', parse_dates=['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the discharge measurements\n",
    "rc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Stage-Discharge Rating Curve and the Best Fit Curve\n",
    "\n",
    "Recall from Notebook 2: $Q = C(H-h_0)^b$.  If we transform the data to log space, we get a linear relationship:\n",
    "\n",
    "$$log(Q) = log(C) + b\\cdot log(h-h_0)$$\n",
    "\n",
    "If we rearrange to the form $y = intercept + slope \\cdot x$, we can use the scipy function for linear regression (`\n",
    "()` from the previous tutorial).\n",
    "\n",
    "Recall the x and y axis parameters are Q and h, respectively, so the linear form of the equation is then: \n",
    "\n",
    "$$log(h-h_0) = slope \\cdot log(Q) + intercept$$\n",
    "\n",
    "The above relationship is linear, so we can use ordinary least squares to find the best fit line (in log-log space), and then transform back to linear space.\n",
    "Note that $h_0$ cannot be fitted this way, and has to be set manually. In this case we can assume $h_0=0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the discharge based on the best fit\n",
    "# parameters found by ordinary least squares above\n",
    "def ols_rc_q(slope, intercept, h, h0):\n",
    "    \"\"\"\n",
    "    Calculate flow (Q) from the linear best fit parameters.\n",
    "        -slope: the `log_slope` calculated above (constant)\n",
    "        -intercept: `log_intercept` calculated above (constant)\n",
    "        -h0 is the same PZF offset used above (constant)\n",
    "        -h is the independent variable\n",
    "    Returns Q, the discharge in m^3/s.\n",
    "    \"\"\"\n",
    "    if slope == 0:\n",
    "        return 0\n",
    "    try:\n",
    "        return np.exp((np.log(h - h0) - intercept) / slope)\n",
    "    except ValueError: \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best-fit line in log-log space\n",
    "# take the logarithm of the measured streamflows and stage\n",
    "h0=0\n",
    "q_log = np.log(rc_df[flow_label] - h0)\n",
    "stage_log = np.log(rc_df[stage_label])\n",
    "\n",
    "# find the parameters describing the linear best fit using ordinary least squares (OLS)\n",
    "log_slope, log_intercept, log_rval, log_pval, log_stderr = st.linregress(q_log, stage_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_range = np.linspace(0.001, 1.5, 100)\n",
    "# put best fit results into a dataframe for plotting\n",
    "# use 0 as the PZF (point of zero flow) (the h0 parameter)\n",
    "bf_df = pd.DataFrame()\n",
    "bf_df['stage'] = stage_range\n",
    "\n",
    "# now as before, apply the `ols_rc_q` function to create the stage-discharge\n",
    "# curve based on the best-fit equation\n",
    "bf_df['best_fit_q'] = [ols_rc_q(log_slope, log_intercept, h, 0.0) for h in stage_range]\n",
    "bf_df.sort_values(by='stage', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Daily Average Discharge\n",
    "\n",
    "From the equation describing the ordinary least squares (OLS) best fit of the measured discharge,\n",
    "calculate daily average flow from daily average water level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_df['RC Q (cms)'] = stage_df['Value'].apply(lambda h: ols_rc_q(log_slope, log_intercept, h, 0))\n",
    "stage_df['Date'] = stage_df.apply(lambda row: pd.to_datetime('{}-{}-{}'.format(row['year'], \n",
    "                                                                               row['month'],\n",
    "                                                                              row['day'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Rating Curve and Resultant Flow Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two plots below are linked.  Check the selection tools, and select points on one plot.  When validating data, it is helpful to be able to link the measurements on the rating curve plot and the daily flow series plot.  Consider how you would you check if the low flows were subject to a shift in the hydraulic control over time?    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output to static HTML file\n",
    "#output_file(\"filename.html\")\n",
    "\n",
    "# customize the tools for interacting with the bokeh plot\n",
    "TOOLS=\"pan,wheel_zoom,reset,hover,poly_select,box_select\"\n",
    "\n",
    "# Bokeh uses a ColumnDataSource data structure to \n",
    "# link plots and make them more interactive\n",
    "# set data sources for plot linking\n",
    "stage_df.reset_index(inplace=True, drop=True)\n",
    "rc_source = ColumnDataSource(data=dict())\n",
    "rc_source.data = rc_source.from_df(rc_df)\n",
    "\n",
    "ts_source = ColumnDataSource(data=dict())\n",
    "ts_source.data = ts_source.from_df(stage_df)\n",
    "\n",
    "#### RATING CURVE PLOT (LET)\n",
    "rc_plot = figure(plot_width=550, plot_height=400,\n",
    "                title='Rating Curve Plot',\n",
    "                tools=TOOLS)\n",
    "\n",
    "# plot the measured discharge points\n",
    "rc_plot.circle(flow_label, stage_label, size=5, color=\"red\", alpha=0.5,\n",
    "              legend_label='Measured Q', source=rc_source)\n",
    "\n",
    "# plot the rating curve based on the OLS best fit\n",
    "rc_plot.line(bf_df['best_fit_q'], bf_df['stage'],\n",
    "             line_color='green', legend_label='OLS Fit')\n",
    "\n",
    "#### DAILY FLOW SERIES PLOT (RIGHT)\n",
    "daily_flow_plot = figure(plot_width=550, plot_height=400, \n",
    "                        x_axis_type='datetime', title='Daily Flow Hydrograph')\n",
    "\n",
    "# # plot the flow series based on the OLS best fit\n",
    "daily_flow_plot.line('Date', 'RC Q (cms)', \n",
    "                    legend_label='OLS-based RC Flow', color='dodgerblue',\n",
    "                    source=ts_source)\n",
    "\n",
    "# plot the daily flow series generated from the manual rating curve\n",
    "daily_flow_plot.circle('Date', flow_label, size=5, color=\"red\", alpha=0.8,\n",
    "              source=rc_source, legend_label='Measured Q')\n",
    "\n",
    "# daily_flow_plot.line('Date', 'RC Q (cms)')\n",
    "# label the axes\n",
    "daily_flow_plot.xaxis.axis_label = 'Date'\n",
    "daily_flow_plot.yaxis.axis_label = 'Flow (m³/s)'\n",
    "daily_flow_plot.legend.location = \"top_left\"\n",
    "rc_plot.xaxis.axis_label = 'Flow (m³/s)'\n",
    "rc_plot.yaxis.axis_label = 'Stage (m)'\n",
    "rc_plot.legend.location = \"bottom_right\"\n",
    "\n",
    "layout = gridplot([[rc_plot, daily_flow_plot]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the results\n",
    "show(layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Characterizing Water Resources using Regional Information\n",
    "\n",
    "Below are a few points to consider regarding the characterization of the water resource over the **long term**.\n",
    "\n",
    "1.  How long has the hydrometric station been operating?  \n",
    "2.  If we're collecting data to support civil engineering design, to assess financial viability of a project, to inform operations, and to quantify the impact of our use of water resources or our modification of natural flow regimes, have we collected enough data?\n",
    "3.  How much data is enough?\n",
    "4.  If we don't have enough, what can we do to better evaluate the water resource?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression of Daily Streamflow\n",
    "\n",
    "![Active WSC Stations in Western Canada](img/wsc_map_view1.png)\n",
    "\n",
    "Water Survey of Canada (WSC) has operated and maintained hydrometric stations across Canada for over 100 years.  If we can find a regional proxy WSC station in **close proximity to our location of interest, with similar basin characteristics** to those of our project, we can correlate daily streamflow between the two locations, ultimately to generate an estimated (or synthetic) long-term flow series for our project location.\n",
    "\n",
    "Typically regressions are done by chronological pairing, which effectively says \"if the flow at the regional (proxy) station was $Q_p$ at time $t$, the flow at our project location at time $t$ will be approximately $C\\cdot Q_p + D$ where $C$ and $D$ are constants.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chronological Pairing (CP)\n",
    "\n",
    "A lot of work goes into finding an appropriate long-term record comparable to our location of interest, but we will assume we have been given a long-term daily flow series to use.\n",
    "\n",
    "1.  Find just the concurrent days of record (days where we have flow recorded for both creeks/rivers).\n",
    "2.  Create a scatter plot where the coordinates of each data point are (flow1, flow2).  It is customary to put the long-term regional station on the x-axis.\n",
    "3.  Determine the equation describing the line of best fit through the data.\n",
    "4.  Apply the best fit line equation to the long-term surrogate record to yield an estimated long-term series for the project location.\n",
    "\n",
    "To further refine this estimate, we can recognize that the mechanisms driving flow across seasons and months can change quite dramatically, and the relationship between the two catchments can also change month-by-month, and/or season by season.  If there is enough data to create seasonal or monthly regressions, we can develop a best-fit equation for each month or season.  The process of steps 2 through 4 then are the same, except we treat each season or month independently.  \n",
    "\n",
    "The above method is referred to as **chronological pairing (CP)**, as it pairs flow that occurs on the same day.  But what if there are timing differences between stations, or what if the spatial variability of precipitation results in flow events that don't coincide in the short term?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empirical Frequency Pairing\n",
    "\n",
    "To eliminate the temporal constraint on timing of events at the daily level, instead of comparing concurrent days, we can compare magnitudes of flows. This method is referred to as Empirical Frequency Pairing (EFP) and is commonly used in British Columbia.  Empirical frequency pairing still uses concurrent records, however it is the ranked flows in each series that are compared, i.e. the data points on an EFP plot are:  \n",
    "\n",
    "$$[(R1_{siteA}, R1_{site_B}), (R2_{siteA}, R2_{siteB}), ..., (Rn_{siteA}, Rn_{siteB})]$$  \n",
    "\n",
    "The steps to derive an estimated long-term flow series for our location of interest are the same (i.e. steps 2-4 above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up a Regression\n",
    "\n",
    "The first step is to import the regional data series and find all of the dates with values for both locations.\n",
    "\n",
    "Previewing the data shows line 1 has information about two distinct parameters.  Where the `PARAM` column value is 1, the `Value` column corresponds to daily discharge ($\\frac{m^3}{s}$ and where the `PARAM` column value is 2, the `Value` column corresonds to daily water level ($m$).  We need to correctly set the header line to index 1 (the second row), so that the headings are read correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_de = pd.read_csv('Stave Daily Avg Flow.csv', header=1, parse_dates=True, index_col='Dates')\n",
    "flow_de['year'] = flow_de.index.year\n",
    "\n",
    "print(flow_de.head(10))\n",
    "\n",
    "flow_de = flow_de.dropna()\n",
    "\n",
    "max_de = flow_de.loc[flow_de.groupby('year')['flow'].idxmax()].copy()\n",
    "max_de.head(10)\n",
    "max_de.to_csv('max_de.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the header row to index 1, tell the function to set the `Date` column as the index.\n",
    "# If you look at the raw csv file, you'll see that there's an information line at the very top,\n",
    "# and the second line (index 1 in programming) is where the column headers are\n",
    "regional_df = pd.read_csv('../../data/Stave Daily Avg Flow.csv', header=0, parse_dates=True, index_col='Date')\n",
    "\n",
    "# select only the discharge data (PARAM == 1)\n",
    "regional_df = regional_df[regional_df['PARAM'] == 1]\n",
    "# \n",
    "# regional_df.columns = ['ID', 'PARAM', 'year', 'month', 'day' 'Regional Flow (m^3/s)', 'SYM']\n",
    "# \n",
    "# preview the data\n",
    "regional_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the date range covered by the regional data\n",
    "print('REGIONAL DATA: Start date = {}, End date = {}'.format(regional_df.index[0], regional_df.index[-1]))\n",
    "\n",
    "# check the date range covered by our data measured at site\n",
    "print('SITE DATA: Start date = {}, End date = {}'.format(stage_df.index[0], stage_df.index[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain Concurrent Data\n",
    "\n",
    "In the previous step, we can see that the regional dataset encompasses the date range of our site data.  To perform a regression, we want to use concurrent data only.   The `concat`, or concatenate, function [documentation can be found here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regional_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe of concurrent data and plot the data\n",
    "# join='inner' says to line up the indices and get the values that are common between the two dataframes\n",
    "# axis=1 says line up columns instead of rows\n",
    "# print(regional_df)\n",
    "stage_df.index = pd.to_datetime(stage_df['Date'])\n",
    "\n",
    "\n",
    "\n",
    "concurrent_df = pd.concat([stage_df, regional_df], join='inner', axis=1)\n",
    "# stage_df.index = pd.to_datetime(stage_df.index)\n",
    "# \n",
    "# filter just the columns we want\n",
    "concurrent_df = concurrent_df[['flow', 'RC Q (cms)']]\n",
    "# not rename the columns to something that is more indicative of the location of each data source\n",
    "concurrent_df.columns = ['Regional_Q', 'Project_Q']\n",
    "\n",
    "# concurrent_df = concurrent_df[concurrent_df['Regional_Q'] < 6.0]\n",
    "\n",
    "concurrent_df.head()\n",
    "\n",
    "concurrent_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the best fit equation\n",
    "slope, intercept, rval, pval, stderr = st.linregress(concurrent_df['Regional_Q'], concurrent_df['Project_Q'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(slope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Regression Plot\n",
    "reg_plot = figure(plot_width=700, plot_height=400,\n",
    "                title='Regression Plot (All Concurrent Data)',\n",
    "                tools=TOOLS)\n",
    "\n",
    "# plot the concurrent daily flows\n",
    "reg_plot.scatter(concurrent_df['Regional_Q'], concurrent_df['Project_Q'],\n",
    "                color='dodgerblue')\n",
    "\n",
    "#plot the best fit\n",
    "q_range = np.linspace(0, concurrent_df['Regional_Q'].max(), 500)\n",
    "intercept = 0.2\n",
    "slope = 0.15\n",
    "reg_line = [slope * q + intercept for q in q_range]\n",
    "\n",
    "reg_plot.line(q_range, reg_line, legend_label='Q = {:.1f}*Qregional + {:.1f}'.format(slope, intercept),\n",
    "             color='firebrick', line_width=2)\n",
    "reg_plot.xaxis.axis_label = 'Regional Station Daily Avg. Flow (m^3/s)'\n",
    "reg_plot.yaxis.axis_label = 'Project Location Daily Avg. Flow (m^3/s)'\n",
    "show(reg_plot)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate Long-Term Daily Flow at Project Location\n",
    "\n",
    "The last step in the process of a long-term flow estimate for our project location is to use the equation of the best fit line (the model) to calculate estimated daily flows over periods where flow was not measured at our project location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt_series = regional_df.copy()[['flow']]\n",
    "lt_series.columns = ['Regional_Q']\n",
    "# map the equation of the best fit line to the regional flow series\n",
    "lt_series['Proj_Q'] = lt_series.apply(lambda q: slope * q + intercept, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the Modelled vs. Measured Flow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_df = pd.concat([stage_df, lt_series], join='inner', axis=1)\n",
    "# keep only the measured and modeled columns\n",
    "mod_df = mod_df[['RC Q (cms)', 'Proj_Q']]\n",
    "# rename the columns to something more intuitive\n",
    "mod_df.columns = ['Measured_Q', 'Modeled_Q']\n",
    "\n",
    "reg_plot = figure(plot_width=700, plot_height=400,\n",
    "                title='Measured vs. Modeled Daily Avg. Flow',\n",
    "                tools=TOOLS, x_axis_type='datetime')\n",
    "\n",
    "reg_plot.line(mod_df.index, mod_df['Measured_Q'], line_width=2,\n",
    "             color='dodgerblue', legend_label='Measured Q')\n",
    "\n",
    "reg_plot.line(mod_df.index, mod_df['Modeled_Q'], line_width=2,\n",
    "             color='darkblue', legend_label='Modeled Q',\n",
    "             line_dash='dashed')\n",
    "\n",
    "show(reg_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the above plot, note general patterns, specific exceptions, and any trends\n",
    "\n",
    "Note the big deviation between the two series in the summer of 2010.  This looks like our model is doing a particularly bad job in the late summer.  How about in other seasons?   How is the model doing at predicting peaks?  \n",
    "\n",
    "How else can we evaluate how the model is fitting the measured data, noting in particular that we are interested in certain ranges of flows, perhaps for generating energy year-round, or supplying a community with drinking water in a dry season?\n",
    "\n",
    "Recall how we plotted the flow duration curve in Notebook 3.  The flow duration curve is particularly useful for focusing on how well the model matches measured data across the range of flows (though extremes are de-emphasized).  \n",
    "\n",
    "### Plot a comparison of Flow Duration Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mod_df['Modeled_Q'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_exceeded = np.linspace(0, 100, 200)\n",
    "mod_df.dropna(inplace=True)\n",
    "msd_quantiles = np.percentile(mod_df['Measured_Q'], pct_exceeded)\n",
    "modeled_quantiles = np.percentile(mod_df['Modeled_Q'], pct_exceeded)\n",
    "\n",
    "fdc_plot = figure(width=700, height=400, title='Measured vs. Modeled Flow Duration Curve')\n",
    "fdc_plot.line(pct_exceeded[::-1], msd_quantiles, legend_label=\"Measured\", line_width=2,\n",
    "             color='dodgerblue')\n",
    "fdc_plot.line(pct_exceeded[::-1], modeled_quantiles, line_width=2,\n",
    "             color='darkblue', legend_label='Modeled', line_dash='dashed')\n",
    "fdc_plot.yaxis.axis_label = 'Flow [m^3/s]'\n",
    "fdc_plot.xaxis.axis_label = 'Percent of Time Exceeded [%]'\n",
    "show(fdc_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate the Long-Term Mean Annual Flow for our Project Location\n",
    "\n",
    "Compare the long term mean annual against the short term, then compare the long-term monthly and annual series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt_mad = lt_series[['Proj_Q']].mean().to_numpy()[0]\n",
    "msd_mean = stage_df[['RC Q (cms)']].mean().to_numpy()[0]\n",
    "lt_series.dropna(inplace=True)\n",
    "lt_median = np.percentile(lt_series['Proj_Q'], 50)\n",
    "msd_median = stage_df[['RC Q (cms)']].median().to_numpy()[0]\n",
    "\n",
    "print('The mean annual flow (MAD) at our project location is {:.1f} m^3/s'.format(lt_mad))\n",
    "print('To compare, the average flow over the measured period was {:.1f} m^3/s'.format(msd_mean))\n",
    "print('The median flow for the long-term period was {:.1f} m^3/s'.format(lt_median))\n",
    "print('To compare, the median flow over the measured period was {:.1f} m^3/s'.format(msd_median))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt_series['year'] = lt_series.index.year\n",
    "annual_series = lt_series[['Proj_Q', 'year']].groupby('year').mean()\n",
    "\n",
    "stage_df['year'] = stage_df.index.year\n",
    "msd_ann = stage_df[['year', 'RC Q (cms)']].groupby('year').mean()\n",
    "\n",
    "plt.plot(annual_series['Proj_Q'], label='LT Modelled')\n",
    "plt.plot([1987, 2017], [lt_mad, lt_mad], label='LT Mean', color='green')\n",
    "plt.scatter(msd_ann.index, msd_ann['RC Q (cms)'], color='red')\n",
    "plt.title('Mean Annual Series')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions for Reflection\n",
    "\n",
    "1.  From our regression plot and from the comparison of measured and estimated daily flow series, what do you think about the quality of our model, i.e. how well does the best fit line approximate the concurrent daily flows (blue dots)?  \n",
    "2.  What range of flow is exceeded 67% OR MORE of the time, how well is this range modelled and what might this flow range be pertinent to?  \n",
    "3.  What could differences in concurrent flows at the two locations be attributable to?  \n",
    "4.  How might we modify our model to capture one of the differences you noted in 3?\n",
    "5.  In the last plot, we see that the ~two years we measured flow at our project location were very close to the long-term mean annual.  What if the two years we happened to measure were 2000 and 2001 and we took these as representative without doing a regression analysis with a long-term regional record?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. A.S. Hamilton & R.D. Moore (2012). Quantifying Uncertainty in Streamflow Records , Canadian Water Resources Journal / Revue canadienne des ressources hydriques, 37:1, 3-21, DOI: 10.4296/cwrj3701865\n",
    "2. Environment Canada (2012).  Hydrometric Manual - Data Computations.  Water Survey of Canada, Weather and Environmental Monitoring Directorate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
