{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 5: Extreme Values, Uncertainty, and Risk\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In Notebook 2, we developed a rating curve from a set of discrete discharge measurements and a continuous record of water level.  At the end of the notebook, you were asked to reflect on how your confidence in the flow predicted by the rating curve changes as a function of water level.  In the near term, our confidence in the rating curve is greatest where we have the most measurements.  Recall that the shape of the rating curve is related to the geometry of the hydraulic control, and that the geometry of the river is constantly evolving.  Without continous validation of the rating curve, we should then be less confident in the rating curve over time, due to this *stream-channel geomorphic response*.  \n",
    "\n",
    "Estimating the volume of water passing a given location during a major flood is necessary for designing infrastructure such as bridge abutments, hydraulic control structures like weirs and dams, and for designing erosion control measures.  In this notebook, we'll take a closer look at the upper end of the rating curve that governs high-magnitude flow events, where by definition we have fewer opportunities to record discrete flow measurements to robustly define a rating curve, and where measurements can be difficult to obtain accurately.  \n",
    "\n",
    "It is often very difficult or impossible to get measurements at high flows due to safety, but also due to timing.  Hydrometric stations are often situated in remote locations, and high flow measurement requires additional planning and consideration for safe work procedures, and unique measurement approaches.  With this understanding of uncertainty in the largest *measured* flows, and recognizing that the highest stage recorded by the hydrometric station is generally substantially greater than the stage corresponding to the largest measured flows, extrapolation is unavoidable.  \n",
    "\n",
    "Beyond extrapolation of the measured flow series at our sites of interest where *in situ* data collection may cover as little as one or two years, the estimation of peak flows for structural design extrapolates event further from measured values.  How can a 1 in 500 year flow be estimated from just two years of data measured on site? The aim of this notebook is to demonstrate the process of flood estimation, building upon the concepts developed in the previous tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1002\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  const JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n    // Clean up Bokeh references\n    if (id != null && id in Bokeh.index) {\n      Bokeh.index[id].model.document.clear();\n      delete Bokeh.index[id];\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim();\n            if (id in Bokeh.index) {\n              Bokeh.index[id].model.document.clear();\n              delete Bokeh.index[id];\n            }\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    const el = document.getElementById(\"1002\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.2.min.js\"];\n  const css_urls = [];\n  \n\n  const inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats as st\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "from bokeh.plotting import figure, output_notebook, show\n",
    "from datetime import timedelta\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Imports\n",
    "\n",
    "### Import the Daily Average Flow Data\n",
    "\n",
    "Daily average flow data provided by the Water Survey of Canada (WSC) for the [Stave River](https://wateroffice.ec.gc.ca/report/historical_e.html?y1Max=1&y1Min=1&scale=normal&mode=Graph&stn=08MH147&dataType=Daily&parameterType=Flow&year=2016) (WSC 08MH147) is saved in `data/notebook_5_data/Stave.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>PARAM</th>\n",
       "      <th>Value</th>\n",
       "      <th>SYM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1983-02-03</th>\n",
       "      <td>08MH147</td>\n",
       "      <td>1</td>\n",
       "      <td>6.59</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983-02-04</th>\n",
       "      <td>08MH147</td>\n",
       "      <td>1</td>\n",
       "      <td>6.40</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983-02-05</th>\n",
       "      <td>08MH147</td>\n",
       "      <td>1</td>\n",
       "      <td>5.93</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983-02-06</th>\n",
       "      <td>08MH147</td>\n",
       "      <td>1</td>\n",
       "      <td>6.21</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983-02-07</th>\n",
       "      <td>08MH147</td>\n",
       "      <td>1</td>\n",
       "      <td>7.04</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID  PARAM  Value  SYM\n",
       "Date                                  \n",
       "1983-02-03  08MH147      1   6.59    A\n",
       "1983-02-04  08MH147      1   6.40  NaN\n",
       "1983-02-05  08MH147      1   5.93  NaN\n",
       "1983-02-06  08MH147      1   6.21  NaN\n",
       "1983-02-07  08MH147      1   7.04  NaN"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../notebook_data/notebook_5_data/Stave.csv', header=1, parse_dates=['Date'], index_col='Date')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note in the csv file the first line tells us that there are two parameters being reported: stage (water level) and flow.  When the `PARAM` column equals 1, the value corresponds to discharge, and where it equals 2 the value corresponds to stage. \n",
    "\n",
    "The SYM column refers to data quality information.  \n",
    "\n",
    "| SYM | Description |\n",
    "|---|---|\n",
    "| A | **Partial Day**: The symbol A indicates that the daily mean value of water level or streamflow was estimated despite gaps of more than 120 minutes in the data string or missing data not significant enough to warrant the use of the E symbol. |\n",
    "| B | **Ice Conditions**: The symbol B indicates that the streamflow value was estimated with consideration for the presence of ice in the stream. Ice conditions alter the open water relationship between water levels and streamflow. |\n",
    "| D | **Dry:** The symbol D indicates that the stream or lake is \"dry\" or that there is no water at the gauge. This symbol is used for water level data only. |\n",
    "| E | **Estimate:** The symbol E indicates that there was no measured data available for the day or missing period, and the water level or streamflow value was estimated by an indirect method such as interpolation, extrapolation, comparison with other streams or by correlation with meteorological data. |\n",
    "| R | **Revised**: The symbol R indicates that a revision, correction or addition has been made to the historical discharge database after January 1, 1989. |\n",
    "\n",
    "(from [Water Survey of Canada](https://wateroffice.ec.gc.ca/contactus/faq_e.html#Q12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select just the flow data (PARAM == 1)\n",
    "flow_df = df[df['PARAM'] == 1].copy()\n",
    "flow_df.loc[:, 'year'] = flow_df.index.year\n",
    "print(flow_df.head())\n",
    "print('')\n",
    "print(\"There are {} values in the Stave River daily flow series.\".format(len(flow_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Data\n",
    "\n",
    "### Plot the Daily Average Flow Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customize the tools for interacting with the bokeh plot\n",
    "TOOLS=\"pan,wheel_zoom,reset,hover,poly_select,box_select\"\n",
    "\n",
    "#### Daily Flow Plot\n",
    "daily_flow_plot = figure(plot_width=700, plot_height=400,\n",
    "                title='Daily Average Flow at Stave River (WSC 08MH147)',\n",
    "                tools=TOOLS, x_axis_type='datetime')\n",
    "\n",
    "daily_flow_plot.line(flow_df.index, flow_df['Value'].to_numpy())\n",
    "show(daily_flow_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annual Maximum Flow Series\n",
    "\n",
    "Estimating return period floods is typically done by deriving a series corresponding to the highest flow recorded in each year.  This series is commonly referred to as the **Annual Maximum Series** (AMS).  It is necessary to use the data collected and managed by others to derive the AMS, and we will consider what this implies as we progress through the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a series representing the maximum flow in each year \n",
    "\n",
    "Derive the AMS.  Also calculate the mean and standard deviation of the series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a series representing the annual maximum daily flow\n",
    "# use the 'groupby' function and get the maximum value from each year\n",
    "max_df = flow_df.loc[flow_df.groupby('year')['Value'].idxmax()].copy()\n",
    "\n",
    "max_df['rank'] = max_df['Value'].rank(ascending=False)\n",
    "max_df['month'] = max_df.index.month\n",
    "max_df['count'] = flow_df.groupby('year').count()['Value'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the mean and standard deviation of the sample\n",
    "mean_q, stdev_q = max_df['Value'].mean(), max_df['Value'].std()\n",
    "start, end = pd.to_datetime(max_df.index.to_numpy()[0]).strftime('%Y-%m-%d'), pd.to_datetime(max_df.index.to_numpy()[-1]).strftime('%Y-%m-%d')\n",
    "print('The daily average flow record goes from {} to {} (n={} years).'.format(start, end, len(max_df)))\n",
    "print('Mean = {:.2f} m^3/s; Standard deviation = {:.2f} m^3/s'.format(mean_q, stdev_q))\n",
    "print('Preview of the Annual Maximum Flow Series:')\n",
    "max_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot a histogram of annual maximum flows\n",
    "\n",
    "If we are going to use statistical methods to estimate return period floods, it is important to consider the shape of the probability distribution, and what that implies about the dominant mechanisms driving peak runoff.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(10,6))\n",
    "max_df.hist('Value', density=True, ax=ax)\n",
    "ax.set_xlabel('Q [m^3/s]')\n",
    "ax.set_ylabel('P(X)')\n",
    "ax.set_title('Annual Maximum Flow Histogram for Stave River')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In flood frequency analysis, the shape of the sample distribution has implications for the way the parameters of a probability distribution are estimated.  The *sample* in this case is the set of annual maximum flows.  The model parameters that we will use to estimate return period floods assume certain characteristics about the data.  Namely, that values are *independent* (the annual maximum in one year does not have an observable effect on the annual maximum of other years), and *identically distributed* (values are derived from the same distribution, and are stationary over time).  In some cases these are reasonable assumptions (short planning horizon, large sample), and in others not (long planning horizon, small sample).  \n",
    "\n",
    "Above it appears as though there are two distinct 'modes', or peaks.  Take a moment to consider what might cause multiple modes in the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In what months does the annual maximum flow typically occur?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "max_df.hist('month', ax=ax, label='All years')\n",
    "\n",
    "complete_max_df = max_df[max_df['count'] > 360]\n",
    "complete_max_df.hist('month', ax=ax, \n",
    "                     label='Complete years',)\n",
    "ax.set_xlabel('Month')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Monthly Count of Annual Maximum Flow Timing')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot above, we can see that the annual maximum flow typically occurs from October to January.  Also note that the orange series has been labelled 'complete years'.  Earlier in this notebook it was pointed out that we have to rely on data collected by others.  Below we'll derive a flood frequency curve for Stave River, and in the process we'll illustrate why **quality review of data is critical**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the record for completeness\n",
    "\n",
    "It is often necessary to use data collected by others.  It is common for datasets to be missing documentation containing important information about quality or limitations of the data.  **Even given well documented data, it is important to do your own quality assurance**.  Below, we will use a daily average flow dataset from the Water Survey of Canada to demonstrate a few ways of validating a dataset.  This is not a manual of quality assurance, but it is used to demonstrate how to incorporate other data to do basic validation.  The general idea is that there are many other environmental signals that have mutual information with the signal of primary interest, in this case daily average streamflow at the Stave River.  For example, would you trust a large spike in runoff if nearby precipitation gauges measured zero rainfall?  Also, how do we deal with missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completeness of Record\n",
    "\n",
    "Find the incomplete years, and consider what we observed above regarding the times of the year the annual maximum flood is more likely to occur.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'] = df.index.values\n",
    "max_df['count'] = flow_df.groupby('year').count()['Value'].values\n",
    "print(max_df[max_df['count'] < 365])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears as though 1984 and 2001 are the years missing the most data.  Does this mean we should exclude these years from the AMS we use as input in the flood frequency analysis?  Are the years where only a few days of records are missing good enough?  \n",
    "\n",
    "Even though we obtained our Stave data from WSC, the organizational body governing hydrometric data collection and standards in Canada, we still must review the data for completeness and quality.  \n",
    "\n",
    "Looking at the printout above showing all of the years with missing days, it might be easy to justify removing 2001 from the dataset, as it is missing more than half of the year, and because we need as large a dataset as possible to buttress our statistical analysis, it is tempting to keep years with only a few missing days.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviewing the Data\n",
    "\n",
    "Consider the possibility that the annual maximum flood is itself the reason the data is missing.  How might we determine this?\n",
    "\n",
    "To start, we can check precipitation records at the closest climate stations.  The monthly count plot shown above suggests the annual maximum typically occurs in October to January, and on the basis that synoptic-scale precipitation events typically occur in winter, perhaps we can determine if a major precipitation event occurred during a period when there is a gap in the data.\n",
    "\n",
    "There are no climate stations from the Meteorological Survey of Canada (MSC) in the immediate vicinity, but there are two within 100 km on either side of the Stave River basin.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whistler is equidistant to the Stave catchment in the opposite direction from the climate station in Hope.\n",
    "whis_df = pd.read_csv('../../data/notebook_3_data/Whistler_348_climate.csv', header=0, index_col='Date/Time', parse_dates=True)\n",
    "whis_df = whis_df[['Total Precip (mm)', 'Total Rain (mm)', 'Snow on Grnd (cm)']]\n",
    "name = 'whis'\n",
    "whis_df.columns = ['{}_total_precip'.format(name), '{}_total_rain'.format(name), \n",
    "                   '{}_snow_on_grnd'.format(name)]\n",
    "\n",
    "# the Laidlaw station is near Hope, BC\n",
    "hope_df = pd.read_csv('../../data/notebook_3_data/Laidlaw_794_climate.csv', header=0, index_col='Date/Time', parse_dates=True)\n",
    "\n",
    "hope_df = hope_df[['Total Precip (mm)', 'Total Rain (mm)', 'Snow on Grnd (cm)']]\n",
    "name = 'hope'\n",
    "hope_df.columns = ['{}_total_precip'.format(name), '{}_total_rain'.format(name), \n",
    "                   '{}_snow_on_grnd'.format(name)]\n",
    "\n",
    "# print(hope_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_data_gaps(gap_df, code):\n",
    "    \"\"\"\n",
    "    Input a timeseries, and return a dataframe summarizing\n",
    "    the start and end times of any gaps in the record.\n",
    "    Note that the code assumes frequency is daily.\n",
    "    \"\"\"\n",
    "    gap_df['Date'] = pd.to_datetime(gap_df.index.values)\n",
    "    gap_df.dropna(subset=['Value'], inplace=True)\n",
    "    deltas = gap_df['Date'].diff()[1:]\n",
    "    # Filter diffs (here days > 1, but could be seconds, hours, etc)\n",
    "    gaps = deltas[deltas > timedelta(days=1)]\n",
    "    # Print results\n",
    "    return gap_df, gaps   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check flow records against precipitation records\n",
    "\n",
    "It is common for historical records to be missing data.  Comparing flow records against precipitation records is just one way of checking to see if the gaps in the record might correspond to peak events.  Code is provided to automatically identify gaps in the record to help you see where they occur.  Note that this doesn't guarantee anything about the conditions in Stave River where we have no data, but it does provide some information with which to build a case for treating the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the precipitation records with the streamflow records\n",
    "conc_df = pd.concat([whis_df, hope_df, flow_df], axis=1, join='outer')\n",
    "conc_df = conc_df[['Value'] + [e for e in hope_df.columns if 'hope' in e] + [e for e in whis_df.columns if 'whis' in e]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conc_df.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import LinearAxis, Range1d\n",
    "\n",
    "# plot flow at Stave vs. precip at the closest climate stations\n",
    "p = figure(width=900, height=400, x_axis_type='datetime')\n",
    "p.line(conc_df.index, conc_df['Value'], alpha=0.8,\n",
    "         legend_label='Stave Flow [m^3/s]', line_color='dodgerblue')\n",
    "\n",
    "# plot on second y axis\n",
    "p.extra_y_ranges = {'precip': Range1d(start=0, end=200)}\n",
    "\n",
    "p.line(conc_df.index, conc_df['hope_total_rain'], alpha=0.3,\n",
    "         legend_label='Hope precipitation [mm]', color='orange', y_range_name='precip')\n",
    "# # p.line(conc_df_trimmed.index, conc_df_trimmed['hope_total_rain'], alpha=0.3,\n",
    "# #          label='Hope precipitation [mm]', color='orange', y_range_name='precip')\n",
    "p.line(conc_df.index, conc_df['whis_total_rain'], alpha=0.3,\n",
    "         legend_label='Whistler precipitation [mm]', color='red', y_range_name='precip')\n",
    "\n",
    "gap_df, gaps = find_data_gaps(conc_df, 'Stave')\n",
    "\n",
    "# plot red bands to illustrate where there are gaps in the record\n",
    "# ind = 0\n",
    "xs, ys = [], []\n",
    "ymin, ymax = 0, conc_df['Value'].max()\n",
    "\n",
    "for i, g in gaps.iteritems():    \n",
    "    gap_start = gap_df.iloc[gap_df.index.get_loc(i) - 1]['Date']\n",
    "    gap_end = gap_start + g\n",
    "    xs.append([gap_start, gap_end, gap_end, gap_start])\n",
    "    ys.append([ymax, ymax, ymin, ymin])\n",
    "\n",
    "p.patches(xs, ys, fill_alpha=0.5,\n",
    "        color='red',legend_label='Gap',\n",
    "         line_alpha=0)\n",
    "\n",
    "p.legend.location = 'top_left'\n",
    "p.legend.click_policy = 'hide'\n",
    "p.xaxis.axis_label = 'Date'#\n",
    "p.yaxis.axis_label = 'Daily Average Flow [m^3/s]'\n",
    "p.y_range = Range1d(0, 580)\n",
    "p.add_layout(LinearAxis(y_range_name='precip', axis_label='Total Precipitation [mm]'), 'right')\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max_df[max_df['count'] < 365])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the interactive plot tools to zoom in on each of the specific years above.  The visibility of series can be toggled by clicking on the corresponding legend item.  \n",
    "\n",
    "If we check each of the incomplete years above, we can see that some of the gaps in the Stave record correspond to large precipitation events at the precipitation stations on either side of the Stave River watershed, suggesting perhaps a gap of just a few days is related to a large event.  Consider how including a year that is missing its true largest event might affect the calculations that follow in developing the flood frequency curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an array of years values of the years you want to exclude\n",
    "drop_years = [1984, 1989, 1991, 1993, 2001]\n",
    "# here, we filter the years we don't want to include in our annual maximum series \n",
    "max_df_filtered = max_df[~max_df.index.year.isin(drop_years)]\n",
    "\n",
    "print('After reviewing the dataset, there are {} years of record in the AMS.'.format(len(max_df_filtered)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Ways of Fitting a Probability Distribution to Measured Data\n",
    "\n",
    "### Method of Moments\n",
    "\n",
    "The method of moments is used to estimate the parameters of a distribution.  The parameters dictate the shape of the PDF, or the curve that approximates the histogram of measured data.  \n",
    "\n",
    "![GEV Densities](img/GEV_densities.png)\n",
    "\n",
    "Source: [Wikipedia](https://en.wikipedia.org/wiki/Generalized_extreme_value_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GEV is a family of distributions, of which the first type (Type 1) is also known as the [Gumbel Distribution](https://en.wikipedia.org/wiki/Generalized_extreme_value_distribution).\n",
    "\n",
    "The Type 1 GEV (GEV1) and Log-Pearson III are commonly used in flood frequency analysis, though we will not go further in the assumptions underlying the types of distributions here, except to say the tail behaviour (which we are interested in because it defines the flows associated with the highest return periods) is governed by the shape parameter ($\\xi$), so it's important.  The GEV1 assumes $\\xi = 0$\n",
    "\n",
    "The probability of non-exceedence is given by the double exponential:\n",
    "\n",
    "$$G(x) = 1 - e^{-e^{-y}}$$\n",
    "\n",
    "The return period ($T$) is the inverse of $G(x)$.  The Gumbel reduced variate is given by:\n",
    "\n",
    "$$y = -\\ln{ \\left( \\ln{ \\left(\\frac{T}{T-1} \\right) } \\right) }$$\n",
    "\n",
    "![Gumbel table](img/gumbel_table.png)\n",
    "\n",
    "From above, we know the length of record is $n=29$, so:\n",
    "\n",
    "| $n$ | $\\bar{y}_n$ | $\\sigma_n$ |\n",
    "|---|---|---|\n",
    "| 29 | 0.5353 | 1.1086 |\n",
    "\n",
    "\n",
    "\n",
    "A tutorial for fitting a Gumbel distribution using Excel is provided [here.](https://serc.carleton.edu/hydromodules/steps/166250.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gumbel_formula(t, ybar_n, xbar, sigma_n, sigma):\n",
    "    \"\"\"\n",
    "    Return an estimated flood quantile based on the Gumbel formula.\n",
    "    The parameters are\n",
    "    described above in Table A-8.\n",
    "    Inputs are:\n",
    "        t: return period (years)\n",
    "        ybar_n: gumbel variate (from Table provided),\n",
    "        xbar: sample mean,\n",
    "        sigma_n: Gumbel standard deviation (from Table provided),\n",
    "        sigma: sample standard deviation\n",
    "    Returns: flow corresponding to return period t\n",
    "    \"\"\"\n",
    "    y = -np.log(np.log(t / (t - 1)))\n",
    "    return  xbar + ((y - ybar_n)/sigma_n) * sigma\n",
    "\n",
    "# define the return periods you want to calculate flows for and their associated probabilities\n",
    "tr = [2.3, 5, 10, 20, 50, 100, 200, 500]\n",
    "pr = [1/t for t in tr]\n",
    "\n",
    "# Apply the gumbel formula to the selected return periods to estimate return period flood.\n",
    "# Look at the format of the inputs to figure out what to replace #value1 and #value2 \n",
    "# below with\n",
    "\n",
    "# q_gumbel = [gumbel_formula(t, #value1, mean_q, #value2, stdev_q) for t in tr]\n",
    "mean_q, stdev_q = max_df['Value'].mean(), max_df['Value'].std()\n",
    "q_gumbel_alldata = [gumbel_formula(t, 0.5353, mean_q, 1.1086, stdev_q) for t in tr]\n",
    "# repeat for the filtered dataset\n",
    "mean1_q, stdev1_q = max_df_filtered['Value'].mean(), max_df_filtered['Value'].std()\n",
    "q_gumbel_filtered = [gumbel_formula(t, 0.5396, mean_q, 1.1255, stdev_q) for t in tr]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the results against the measured data\n",
    "\n",
    "The appropriate method of fitting a probability distribution to measured data has been argued for decades.\n",
    "\n",
    "In the case of Stave River, we have 34 years of record.  It's intuitive to think that the highest flow measured in that time has a probability of $\\frac{1}{34}$, and a return period of 34 years, but this is incorrect.  Various adjustments to the way in which probabilities are assigned have been proposed over the decades (reference). These adjustments are referred to as *plotting positions*, and their aim is to apply a transformation to the measured data such that the data form a straight line.  If the data fall on the line exactly, the plotting position is said to be unbiased.  \n",
    "\n",
    "A general plotting position formula is as follows:\n",
    "\n",
    "$$\\frac{1}{T} = P = \\frac{m - a}{n + 1 - 2a}$$\n",
    "\n",
    "Where $m$ is the rank (largest value = 1), $n$ is the sample size (number of years), and $a$ is some empirical value.  \n",
    "\n",
    "The table below shows something of a history of plotting positions (from Cunnane, 1977):\n",
    "\n",
    "| Source | Value of $a$ |\n",
    "|---|---|\n",
    "| Hazen (1914) | 0.5 |\n",
    "| California dept. of Public Works (1923) | $\\frac{(i-1)}{N}$, $\\frac{i}{N}$ |\n",
    "| Weibull (1939) | 0 |\n",
    "| Beard (1943), Gumbel (1943), Kimball (1946) | 0.31 |\n",
    "| Blom (1958) | $\\frac{3}{8}$ |\n",
    "| Tukey (1962) |  $\\frac{1}{3}$ |\n",
    "| Gringorten (1963) | 0.44 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we need to sort the measured data by rank\n",
    "# and calculate probabilities associated with the measured data.\n",
    "# The largest flood value should have rank 1\n",
    "max_df = max_df.sort_values('rank')\n",
    "\n",
    "# calculate the probabilty P and return period Tr\n",
    "max_df['P'] = max_df['rank'] / (len(max_df) + 1)\n",
    "max_df['Tr'] = [1/e for e in max_df['P']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting_position(m, a, n):\n",
    "    \"\"\"\n",
    "    Return an adjusted plotting position (probability)\n",
    "    based on the rank m, the plotting position a, and the length of record n.\n",
    "    \"\"\"\n",
    "    return (m - a) / (n + 1 - 2 * a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
    "plt.plot(tr, q_gumbel_alldata, label=\"Gumbel (All data)\",\n",
    "        color='dodgerblue')\n",
    "plt.plot(tr, q_gumbel_filtered, label=\"Gumbel (filtered)\",\n",
    "        color='dodgerblue', linestyle='--')\n",
    "plt.scatter(max_df['Tr'], max_df['Value'], \n",
    "            label='Measured Annual Maxima (all data)', c='red')\n",
    "\n",
    "ax.set_title('Stave River Annual Maximum Daily Average Flow')\n",
    "ax.set_xlabel('Return Period (years)')\n",
    "ax.set_ylabel('Q [m^3/s]')\n",
    "# ax.set_xlim(0,200)\n",
    "plt.xscale('log')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Pearson III Distribution\n",
    "\n",
    "A distribution commonly used for estimating return period floods in BC is the Log-Pearson III distribution.  Here we will plot it against the GEV1 previously developed, and we'll take a look at the effects of our data review, where we'll plot both the GEV and LP3 using the entire dataset (without excluding any years) as well as a filtered dataset where we remove years where there is some likelihood the annual peak was missing from the record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_LP3(values, Tr):\n",
    "    \"\"\"\n",
    "    Fit a log-Pearson III distribution to a sample of extreme values\n",
    "    given a desired set of return periods (Tr).\n",
    "    \"\"\"    \n",
    "    q_skew = st.skew(values)\n",
    "    log_skew = st.skew(np.log10(values))\n",
    "    \n",
    "    # calculate the CDF\n",
    "    z = pd.Series([st.norm.ppf(1-(1/return_p)) if return_p != 1 else st.norm.ppf(1-(1/return_p + 0.1)) for return_p in tr])\n",
    "    lp3 = 2 / log_skew * (np.power((z - log_skew/6)*log_skew/6 + 1, 3)-1)\n",
    "    lp3_model = np.power(10, np.mean(np.log10(values)) + lp3 * np.std(np.log10(values)))\n",
    "    return lp3_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now set up the filtered AMS series to calculate the LP3 distribution\n",
    "max_df_filtered = max_df_filtered.sort_values('rank')\n",
    "\n",
    "# calculate the probabilty P and return period Tr\n",
    "max_df_filtered['P'] = max_df_filtered['rank'] / (len(max_df_filtered) + 1)\n",
    "max_df_filtered['Tr'] = [1/e for e in max_df_filtered['P']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp3_model = calculate_LP3(max_df['Value'], tr).to_numpy()\n",
    "lp3_model_filtered = calculate_LP3(max_df_filtered['Value'], tr).to_numpy()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12,8))\n",
    "\n",
    "plt.plot(tr, q_gumbel_alldata, label=\"Gumbel (34 years)\", color='dodgerblue')\n",
    "plt.plot(tr, q_gumbel_filtered, label=\"Gumbel (29 years)\",\n",
    "        color='dodgerblue', linestyle='--')\n",
    "plt.plot(tr, lp3_model, label=\"LP3 (34 years)\", color='green')\n",
    "plt.plot(tr, lp3_model_filtered, label=\"LP3 (29 years)\", color='green', \n",
    "         linestyle='--')\n",
    "\n",
    "plt.scatter(max_df['Tr'].to_numpy(), max_df['Value'].to_numpy(), \n",
    "            label='Measured Annual Maxima', c='red')\n",
    "\n",
    "ax.set_title('Stave River Annual Maximum Daily Average Flow')\n",
    "ax.set_xlabel('Return Period (years)')\n",
    "ax.set_ylabel('Flow [m^3/s]')\n",
    "plt.xscale('log')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk and Uncertainty in Design \n",
    "\n",
    "It is often the case that engineers are asked to extrapolate well beyond the range of measured data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Considerations regarding Distribution Selection\n",
    "\n",
    "[Bulletin 17B](https://water.usgs.gov/osw/bulletin17b/dl_flow.pdf) of the USDOE Hydrology Subcommittee of the Water Resources Council recommends the Log-Pearson Type III as the distribution for defining annual flood series.  Consider what Bulletin 17B says about sample size and extrapolation.  \n",
    "\n",
    "The annual maximum flood events are assumed to be **independent and identically distributed (IID).**\n",
    "\n",
    "Parts D and E (pg 7 in Bulletin 17B) discuss **mixed populations** and **measurement error**, respectively. These will be addressed sequentially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixed Populations\n",
    "\n",
    "Here, we'll briefly revisit the histogram of annual maximum flows we saw near the beginning of this notebook.  \n",
    "\n",
    "The mechanisms driving flood events may not be homogeneous.  For instance, the annual maximum flow may be due to snowmelt, precipitation, or a combination of the two. Recall from above (and please forgive my bad sketch of a PDF):\n",
    "\n",
    "![Bimodal Distribution](img/bimodal_diagram.png)\n",
    "\n",
    "How might we check if there is a clear distinction between annual floods derived from different types of event?  High flows generated by snowmelt and those generated by early winter ['pineapple express'](https://oceanservice.noaa.gov/facts/pineapple-express.html) precipitation events are unique, and are likely the reason the probability distribution of the AMS at Stave River shows a bimodal distribution, however **the sample size is small, so the shape of the distribution could also be due to sampling variability**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we check to understand the processes which drive the largest magnitude runoff (flow) events?\n",
    "\n",
    "* Check time of year\n",
    "* Compare precipitation and snow values to peak events\n",
    "* Look at temperature records\n",
    "\n",
    "There is a [large body of literature](https://scholar.google.ca/scholar?q=mixed+modes+in+flood+frequency+analysis%27&hl=en&as_sdt=0&as_vis=1&oi=scholart) investigating the treatment of mixed modes in flood frequency analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measurement Error & Sensitivity\n",
    "\n",
    "In the introduction, we laid out the extent to which we rely on extrapolation in estimating return period flows.  This question is analogous to the assumption that the error in measurement is a random variable following some distribution.  Considering this random error exists in our measurements, what happens to the LP3 fit if we change any of the peak values by some amount?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question for Reflection\n",
    "\n",
    "Recall the discussion in the previous notebooks concerning extrapolation.  Return periods in the range of 100 and 200 years are commonly used for input design parameters in hydraulic structures, and ultimately the design values reflect a tradeoff between risk (environmental, financial, worker safety) and construction costs.  \n",
    "\n",
    "Provide a brief discussion (500 words maximum) about the uncertainty introduced at various levels in deriving the estimate of the 100 year return period for Stave River.  Consider the difference (in this case) in the estimated 100 year return period flow based on using all the data vs. removing some years from the record, and compare it to the difference between the estimates generated between the GEV and LP3 distributions.  Consider how measurement uncertainty might affect the 100 year return flood estimate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
